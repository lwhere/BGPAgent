nohup: ignoring input
  0%|          | 0/51 [00:00<?, ?it/s]  2%|▏         | 1/51 [00:16<13:39, 16.39s/it]  4%|▍         | 2/51 [00:34<14:04, 17.24s/it]  6%|▌         | 3/51 [00:50<13:20, 16.67s/it]  8%|▊         | 4/51 [01:04<12:12, 15.59s/it] 10%|▉         | 5/51 [01:31<15:18, 19.96s/it] 12%|█▏        | 6/51 [01:47<13:58, 18.63s/it] 14%|█▎        | 7/51 [02:04<13:15, 18.07s/it] 16%|█▌        | 8/51 [02:22<12:57, 18.07s/it] 16%|█▌        | 8/51 [02:44<14:46, 20.61s/it]
Traceback (most recent call last):
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 363, in <module>
    main(input_file_path, cache_path, asrank_api_result_path, model_series)
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 304, in main
    llm_user_content = get_llm_user_content(f"{pure_as_path_question}{as_path}.", model_series)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 126, in get_llm_user_content
    llama3_70b_value = get_client_chat_completion_value(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 44, in wrapper
    raise e
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 30, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 64, in get_client_chat_completion_value
    chat_completion = client.chat.completions.create(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 289, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1225, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 920, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1003, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1051, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1003, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1051, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1018, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01hsebnn0vf5f87j2gy8s2ebnw` on tokens per minute (TPM): Limit 6000, Used 6254, Requested 305. Please try again in 5.599s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
