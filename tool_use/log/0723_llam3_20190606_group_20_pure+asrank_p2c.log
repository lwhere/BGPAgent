nohup: ignoring input
  0%|          | 0/26 [00:00<?, ?it/s]  4%|▍         | 1/26 [00:18<07:33, 18.15s/it]  8%|▊         | 2/26 [00:31<06:01, 15.05s/it] 12%|█▏        | 3/26 [00:47<05:58, 15.57s/it] 15%|█▌        | 4/26 [01:34<10:13, 27.90s/it] 19%|█▉        | 5/26 [02:05<10:09, 29.01s/it] 23%|██▎       | 6/26 [02:40<10:26, 31.32s/it] 27%|██▋       | 7/26 [03:01<08:49, 27.88s/it] 31%|███       | 8/26 [03:22<07:43, 25.74s/it] 35%|███▍      | 9/26 [03:40<06:35, 23.29s/it] 38%|███▊      | 10/26 [04:15<07:11, 26.97s/it] 42%|████▏     | 11/26 [04:33<06:03, 24.25s/it] 42%|████▏     | 11/26 [04:57<06:45, 27.03s/it]
Traceback (most recent call last):
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 363, in <module>
    main(input_file_path, cache_path, asrank_api_result_path, model_series)
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 326, in main
    llm_user_content = get_llm_user_content(f"{question_type_dict['combine_asrank_question']}.As path: {as_path}.Asrank algorithm inference result: {asrank_user_content}", model_series)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 126, in get_llm_user_content
    llama3_70b_value = get_client_chat_completion_value(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 44, in wrapper
    raise e
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 30, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yyc/BGP-Woodpecker/BGPAgent/tool_use/as_relationship_inference.py", line 64, in get_client_chat_completion_value
    chat_completion = client.chat.completions.create(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 289, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1225, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 920, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1003, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1051, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1003, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1051, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/miniconda3/lib/python3.11/site-packages/groq/_base_client.py", line 1018, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01hyweqwnwfr4aycbjzft19dtg` on tokens per minute (TPM): Limit 6000, Used 6468, Requested 577. Please try again in 10.459s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
